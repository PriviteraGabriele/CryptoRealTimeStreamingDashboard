# üîó Crypto Real-Time Streaming Dashboard

## üì© Problematica

Nel mondo delle criptovalute, i dati di mercato sono generati in quantit√† massicce e in tempo reale. Tuttavia, per analizzarli efficacemente e in modo tempestivo servono strumenti capaci di:

- raccogliere i dati da fonti esterne in streaming,  
- processarli rapidamente,  
- salvarli in un sistema scalabile e interrogabile,  
- visualizzarli in dashboard intuitive.  

Molti strumenti esistenti offrono queste funzioni separatamente, ma pochi offrono una pipeline **completa e personalizzabile** per progetti di analisi in tempo reale.

## üåü Obiettivo del progetto

Realizzare una pipeline di **data streaming real-time** che raccolga dati di **trade pubblici** da Bybit (una delle principali piattaforme crypto), li invii a Kafka, li processi con Spark Streaming, li salvi su Elasticsearch e li visualizzi su Kibana. In questo modo:

- puoi analizzare **in tempo reale** il comportamento di mercato,  
- identificare pattern, volumi, anomalie,  
- creare dashboard dinamiche per supportare decisioni data-driven.  

Inoltre, √® stata integrata una componente **di machine learning** per **predire la direzione del mercato** in base alle caratteristiche di ogni trade.

## üß† Modulo Machine Learning

√à stato aggiunto un sistema predittivo per classificare ogni transazione come **probabile aumento (1)** o **diminuzione (0)** del prezzo:

### üì¶ Dataset

Durante la fase iniziale, ho valutato l'utilizzo di **dataset esistenti** per addestrare il modello. Tuttavia, essendo il mio stream di dati basato su **transazioni in tempo reale di Bybit**, con struttura e frequenza altamente specifiche, ho deciso di generare un dataset **customizzato** per rispecchiare fedelmente il comportamento del mio sistema.

- I dati storici sono stati **esportati direttamente da Elasticsearch** (dove vengono salvati dallo stream Spark) in formato `.json`
- √à stato effettuato un **preprocessing** per creare un file `CSV` compatibile, estrapolando i seguenti campi per ogni trade:
  - `price`
  - `quantity`
  - `is_buy` (booleano 1/0)
  - `target` (etichetta 1 o 0 per classificazione)
 
### üî¢ Come abbiamo generato la colonna target

L‚Äôobiettivo era classificare ogni trade in base alla **direzione del prezzo**:

- 1 se il prezzo **aumenta** dopo il trade
- 0 se il prezzo **diminuisce o rimane uguale**

#### üì¶ Passaggi:
1. **Ordinamento temporale**: i trade sono stati ordinati usando il timestamp original_time (formato ISO 8601).
2. **Confronto dei prezzi**: per ogni riga, abbiamo confrontato il prezzo con quello della riga successiva.
3. **Assegnazione del target**:
   
```python
   if next_price > current_price:
       target = 1
   else:
       target = 0
```
4. **Pulizia finale**: l‚Äôultima riga, non avendo un valore successivo, √® stata eliminata.

In questo modo, √® stato creato un dataset customizzato utilizzabile per l‚Äôaddestramento.

### ü§ñ Addestramento

Il modello utilizzato √® un 'Random Forest Classifier' implementato con 'scikit-learn'. L‚Äôaddestramento avviene all‚Äôinterno di un container Docker isolato, utilizzando il dataset generato tramite preprocessing personalizzato, eseguendo il comando:
```bash
docker compose --profile train run --rm ml-trainer
```
### üöÄ Servizio di inferenza

- √à stato sviluppato un **microservizio FastAPI** per esporre un endpoint `/predict`
- Ogni nuova transazione viene inviata a questo endpoint da Spark, che riceve in risposta la **predizione** in tempo reale

### ‚öôÔ∏è Integrazione con Spark

- Lo script Spark √® stato aggiornato per **chiamare il microservizio ML** per ogni batch e **arricchire i dati Elasticsearch** con il campo `"prediction"`

## üõ†Ô∏è Tecnologie utilizzate e motivazioni

| Tecnologia                 | Ruolo nel progetto                          | Perch√© √® stata scelta                                              |
|---------------------------|---------------------------------------------|--------------------------------------------------------------------|
| **Bybit WebSocket API**   | Sorgente dei dati (real-time public trades) | Fornisce dati affidabili, aggiornati al secondo, via WebSocket     |
| **Apache Kafka**          | Ingestione e buffer dei dati                | Alta disponibilit√†, scalabilit√† e gestione dello stream            |
| **Apache Spark Streaming**| Processamento in tempo reale                | Supporta trasformazioni avanzate e scrittura su Elasticsearch      |
| **Elasticsearch**         | Archiviazione e indicizzazione dei dati     | Ricerca veloce, supporto nativo per time series e analisi testuali |
| **Kibana**                | Visualizzazione dei dati                    | Dashboard potenti e real-time integrate con Elasticsearch          |
| **FastAPI**               | Servizio di inferenza per ML                | Leggero, veloce e perfetto per microservizi REST                   |
| **scikit-learn**          | Addestramento del modello predittivo        | Semplice ed efficace per classificazione supervisionata            |
| **Docker**                | Isolamento e portabilit√† dei servizi        | Facilita il deploy locale e la gestione dei container              |
| **Python**                | Linguaggio per ingestione e orchestrazione  | Semplice, potente, ampiamente usato nella data engineering         |

## üîÑ Architettura e flusso dati

![Schema del flusso](img/Flusso.png)

## üîé Esempi di utilizzo e visualizzazioni

- **Andamento del prezzo per simbolo nel tempo**
- **Andamento del Prezzo Mediano per Transazioni Buy/Sell nel Tempo**
- **Distribuzione tra long/short**
- **Distribuzione delle criptovalute pi√π attive**
- **Volume medio per prediction**
- **Andamento del prezzo vs prediction**

![Esempio Dashboard](img/EsempioDashboard.png)

## üí° Conclusione e possibilit√† di espansione

Il progetto mostra come si pu√≤ creare una **pipeline di streaming scalabile e flessibile**, utile per:

- il monitoraggio in tempo reale del mercato crypto,  
- l‚Äôanalisi dei trend e delle fluttuazioni,  
- lo sviluppo di modelli predittivi per supporto decisionale  

üîÆ **Possibili estensioni future**:

- Addestramento continuo del modello (online learning)
- Integrazione con Telegram per alert automatici
- Analisi storiche pi√π avanzate (ex: rolling windows)
- Integrazione con ulteriori indicatori tecnici (RSI, MACD, ecc.)

---

### üß™ Comandi di utilizzo

```bash
docker compose build
docker compose --profile train run --rm ml-trainer  # Allena il modello ML
docker compose up -d                                # Avvia tutti i servizi
```
